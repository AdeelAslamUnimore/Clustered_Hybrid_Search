{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average match for folder 20: 0.9688666666666667\n",
      "Average match for folder 40: 0.9804666666666666\n",
      "Average match for folder 60: 0.9827333333333333\n",
      "Average match for folder 100: 0.9848000000000001\n",
      "Average match for folder 200: 0.9869333333333333\n",
      "Average match for folder 300: 0.9869999999999999\n",
      "Average match for folder 400: 0.9872\n",
      "Average match for folder 500: 0.9874666666666666\n",
      "Average match for folder 600: 0.9878666666666666\n",
      "Average match for folder 800: 0.9878\n",
      "Average match for folder 1000: 0.9877333333333332\n",
      "Average match for folder 1200: 0.9878\n",
      "Average match for folder 1500: 0.9880666666666666\n",
      "Average match for folder 1700: 0.9889333333333333\n",
      "Average match for folder 1900: 0.9889333333333333\n",
      "Average match for folder 2100: 0.9896666666666667\n",
      "Summary of averages has been written to /data3/Adeel/Data4/InstantResultCheck/SummaryResults.csv\n",
      "0.9689, 0.9805, 0.9827, 0.9848, 0.9869, 0.9870, 0.9872, 0.9875, 0.9879, 0.9878, 0.9877, 0.9878, 0.9881, 0.9889, 0.9889, 0.9897\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "def process_file(filename, ground_truth_folder, algorithm_folder):\n",
    "    ground_truth_file = os.path.join(ground_truth_folder, filename)\n",
    "    algorithm_file = os.path.join(algorithm_folder, filename)\n",
    "    \n",
    "    if not os.path.exists(algorithm_file):\n",
    "        # If the algorithm file doesn't exist, return None\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV files\n",
    "        ground_truth_df = pd.read_csv(ground_truth_file)\n",
    "        algorithm_df = pd.read_csv(algorithm_file)\n",
    "        \n",
    "        # Remove duplicates from the algorithm dataframe\n",
    "        algorithm_df = algorithm_df.drop_duplicates()\n",
    "        \n",
    "        # Get the first 10 records of the ground truth file\n",
    "        ground_truth_first_10 = ground_truth_df.head(10)\n",
    "        \n",
    "        # Use set intersection for faster matching\n",
    "        algorithm_ids = set(algorithm_df['ID'])\n",
    "        ground_truth_ids = set(ground_truth_first_10['ID'])\n",
    "        match_count = len(algorithm_ids.intersection(ground_truth_ids))\n",
    "        \n",
    "        return {'query': filename, 'match': match_count}\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur while processing\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder(ground_truth_folder, algorithm_folder, output_file):\n",
    "    # List all files in the ground truth folder\n",
    "    ground_truth_files = os.listdir(ground_truth_folder)\n",
    "    \n",
    "    # Check if the output directory exists, and create it if not\n",
    "    output_directory = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Create a partial function with the folders already specified\n",
    "    process_file_partial = partial(process_file, \n",
    "                                   ground_truth_folder=ground_truth_folder,\n",
    "                                   algorithm_folder=algorithm_folder)\n",
    "    \n",
    "    # Use all available CPU cores\n",
    "    num_processes = cpu_count()\n",
    "    \n",
    "    # Create a pool of worker processes\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Process files in parallel\n",
    "        results = pool.map(process_file_partial, ground_truth_files)\n",
    "    \n",
    "    # Filter out None results and create DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to the output CSV file\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    #print(f\"Results have been written to {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def main():\n",
    "    ground_truth_folder = 'GROUND_TRUTH'\n",
    "    algorithm_base_folder = 'RESULTFOLDER'\n",
    "    output_base_folder = 'FOLDERTOHOLDINTERMEDIATEVALUES'\n",
    "    # 40, 60, 100, 200, 300, 400, 500, 600, 800, 1000, 1200, 1500,\n",
    "    #                  1700, 1900, 2100, 2400, 2700, 3000, 3200, 3500, 3700, 4000\n",
    "    # Folder suffixes to iterate over\n",
    "    folder_suffixes = [20,40, 60, 100, 200, 300, 400, 500, 600, 800, 1000, 1200, 1500,\n",
    "                    1700, 1900, 2100]\n",
    "    \n",
    "    all_results = []\n",
    "    result_strings = []  # List to store results\n",
    "    for folder_suffix in folder_suffixes:\n",
    "        algorithm_folder = os.path.join(algorithm_base_folder, str(folder_suffix))\n",
    "        output_file = os.path.join(output_base_folder, f'Res{folder_suffix}.csv')\n",
    "        \n",
    "        # Process the folder and get the results\n",
    "        results_df = process_folder(ground_truth_folder, algorithm_folder, output_file)\n",
    "        \n",
    "        # Calculate the average match for the current folder\n",
    "        average_match = results_df['match'].mean()\n",
    "        # print(f\"Average match for folder {folder_suffix}: {average_match/10}\")\n",
    "        \n",
    "        # Append to overall results\n",
    "        all_results.append({'folder': folder_suffix, 'average_match': average_match})\n",
    "        result_strings.append(f\"{average_match/10:.4f}\")  # Store the result with formatting\n",
    "\n",
    "    # Save all average matches in a single summary file\n",
    "    summary_df = pd.DataFrame(all_results)\n",
    "    summary_file = os.path.join(output_base_folder, 'SummaryResults.csv')\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"Summary of averages has been written to {summary_file}\")\n",
    "    print(\", \".join(result_strings))\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['predicate'] = df['predicate'].str.strip(';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
